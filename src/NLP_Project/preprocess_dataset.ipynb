{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f735cc35-7762-4152-9ad7-d5d798ff7661",
   "metadata": {},
   "source": [
    "# OpenCampus NLP Project\n",
    "## Tweet Generator for famous Twitter personalities\n",
    "-----------\n",
    "This notebook preprocesses the Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb2299-5456-4411-a087-12a8200eecf7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e9a5c-e75b-4dad-99c9-dffab5ccad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09e556-6f23-4b22-9e17-70074c66079d",
   "metadata": {},
   "source": [
    "## Prepare the dataset for Training\n",
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7132e3-b38f-4ef9-808e-2a02261125bb",
   "metadata": {},
   "source": [
    "First we download our custom HuggingFace (HF) dataset. The dataset can be found on our [HuggingFace site](https://huggingface.co/datasets/ML-Projects-Kiel/tweetyface). It contains Tweets from English and German Twitter users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbf9f8-e3e3-4944-8c68-c579590f0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ML-Projects-Kiel/tweetyface\", \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645a965-77e7-4cf0-a411-3594fffc9f14",
   "metadata": {},
   "source": [
    "The dataset already is split into a training and validation subset. It contains no test data, because the text generation task does not require test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190d06a-eb32-41db-85b3-415705ecd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f9a20-271d-4545-b6d9-245cee11cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4760ceb-53f9-47ba-a045-2253e8834e4d",
   "metadata": {},
   "source": [
    "### Preprocess the text\n",
    "#### Filter characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48efb97-2eb8-4c70-bb36-3ab78020a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_text_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb19156-1c2d-4625-a208-a1faed054c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(element: dict[str] | dict[list]) -> dict[str] | dict[list]:\n",
    "    if isinstance(element, datasets.arrow_dataset.Batch):\n",
    "        # Input is of form dict[list]\n",
    "        element[\"text\"] = [f\"Tweet: {txt}\" for txt in element[\"text\"]]\n",
    "    else:\n",
    "        # Input is of form dict[str]\n",
    "        element[\"text\"] = f\"Tweet: {element['text']}\"\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a46ae6-46c9-4849-a0e0-92e676321900",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed = dataset.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab06bb-903e-49db-8d60-63af31caf34d",
   "metadata": {},
   "source": [
    "#### Filter Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffaeaa-f19e-4a63-b712-35ff7929829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed = dataset_processed.filter(lambda row: len(row[\"text\"]) > min_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260fa4e-5e58-42a2-8725-ba96c7002942",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291706c4-014c-41fa-b635-e83a6311c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79861e-a095-4701-b78b-ea0f7e2179cb",
   "metadata": {},
   "source": [
    "### Create Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682c69b-f383-4ad0-a760-c7059708a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(element: dict[str] | dict[list], userlist: list[str]) -> dict[str] | dict[list]:\n",
    "    if isinstance(element, datasets.arrow_dataset.Batch):\n",
    "        # Input is of form dict[list]\n",
    "        element[\"text_prompt\"] = [\n",
    "            f\"User: {userlist[label]}\\nTweet: {txt}\"\n",
    "            for txt, label in zip(element[\"text\"], element[\"label\"])\n",
    "        ]\n",
    "    else:\n",
    "        # Input is of form dict[str]\n",
    "        element[\"text_prompt\"] = f\"User: {userlist[element['label']]}\\nTweet: {element['text']}\"\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b849031-d859-4672-ab7b-7069f5a7623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_features = dataset[\"train\"].features[\"label\"].names  # Create List with all users\n",
    "create_prompt_partial = functools.partial(create_prompt, userlist=full_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421a362-c347-442f-8a91-0dcd7300fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc_prompt = dataset_processed.map(create_prompt_partial, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7852b-c291-4022-aeda-2986d7240599",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc_prompt[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e2097-97ec-4296-9815-0ba0aacd878f",
   "metadata": {},
   "source": [
    "### Filter the Users\n",
    "The full dataset contains more users than we want to use for the first trials. Therefore we will reduce the number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473e97c-e30b-464b-b94a-ddcff67e9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_features = [\n",
    "    \"KBHD\",\n",
    "    \"elonmusk\",\n",
    "    \"alyankovic\",\n",
    "    \"GretaThunberg\",\n",
    "    \"BarackObama\",\n",
    "    \"Trevornoah\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665fea9-cffa-4fcf-a51f-ca6117d3011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_proc_prompt_filter = dataset_proc_prompt.filter(\n",
    "    lambda row: full_features[row[\"label\"]] in short_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985f535-36d6-4d95-9562-8103d4da313c",
   "metadata": {},
   "source": [
    "### Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf142d-2a4f-48b5-b210-95679fcfcb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_path = os.path.join(\"data\", \"feature\", \"final_dataset\")\n",
    "levels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73a121-a54a-487e-a888-e4f0f97a2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = Path(os.path.abspath(\"\")).parents[levels - 1]\n",
    "feature_dir = os.path.join(parent_path, feature_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458744f8-620f-4d9e-9317-535d6187c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_proc_prompt_filter.save_to_disk(feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3f118-b219-4323-ae0f-f46430eb49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_proc_prompt_filter = load_from_disk(feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d17bb9-a4e0-4a4d-9e9b-35ea5ae9d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datset_proc_prompt_filter[\"train\"]\n",
    "dataset_val = datset_proc_prompt_filter[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead006b3-e785-46a4-aa38-f67d1083f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f2f7e-1f4e-4477-985f-6de337d6c6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCampusNLP",
   "language": "python",
   "name": "opencampusnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
