{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f735cc35-7762-4152-9ad7-d5d798ff7661",
   "metadata": {},
   "source": [
    "# OpenCampus NLP Project\n",
    "## Tweet Generator for famous Twitter personalities\n",
    "-----------\n",
    "This notebook preprocesses the Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb2299-5456-4411-a087-12a8200eecf7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e9a5c-e75b-4dad-99c9-dffab5ccad9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea73178-20b6-404f-8de5-8ec13e4bf1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.monospace\"] = [\"DejaVu Sans Mono\"]\n",
    "plt.rcParams[\"font.family\"] = \"monospace\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09e556-6f23-4b22-9e17-70074c66079d",
   "metadata": {},
   "source": [
    "## Prepare the dataset for Training\n",
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7132e3-b38f-4ef9-808e-2a02261125bb",
   "metadata": {},
   "source": [
    "First we download our custom HuggingFace (HF) dataset. The dataset can be found on our [HuggingFace site](https://huggingface.co/datasets/ML-Projects-Kiel/tweetyface). It contains Tweets from English and German Twitter users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbf9f8-e3e3-4944-8c68-c579590f0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"ML-Projects-Kiel/tweetyface\", \"english\", download_mode=\"force_redownload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645a965-77e7-4cf0-a411-3594fffc9f14",
   "metadata": {},
   "source": [
    "The dataset already is split into a training and validation subset. It contains no test data, because the text generation task does not require test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190d06a-eb32-41db-85b3-415705ecd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f9a20-271d-4545-b6d9-245cee11cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4760ceb-53f9-47ba-a045-2253e8834e4d",
   "metadata": {},
   "source": [
    "### Preprocess the text\n",
    "\n",
    "#### Remove retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "userlist = dataset[\"train\"].features[\"label\"].names\n",
    "all_data_pd = datasets.concatenate_datasets(\n",
    "    [dataset[\"train\"], dataset[\"validation\"]]\n",
    ").to_pandas()\n",
    "all_data_pd[\"user\"] = [userlist[label] for label in all_data_pd[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.concat(\n",
    "    [\n",
    "        all_data_pd.groupby(\"user\").size(),\n",
    "        all_data_pd[all_data_pd[\"text\"].str.contains(\"^RT .*\")].groupby(\"user\").size(),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "stats.columns = [\"all_tweets\", \"retweets\"]\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove retweets\n",
    "dataset_without_rt = dataset.filter(\n",
    "    lambda row: not bool(re.search(\"^RT .*\", row[\"text\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d32ba",
   "metadata": {},
   "source": [
    "#### Filter characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708d854-7d7b-49a9-bc84-bfa6e3134338",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = random.sample(range(0, dataset_without_rt[\"train\"].num_rows), 10)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f6e12-9761-403b-82db-d751cb040771",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [12784, 15988, 14475, 10926, 23442, 6853, 22511, 18022, 13039, 22725]\n",
    "dataset_without_rt[\"train\"][ids][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb19156-1c2d-4625-a208-a1faed054c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(element: dict[str] | dict[list]) -> dict[str] | dict[list]:\n",
    "    if isinstance(element, datasets.arrow_dataset.Batch):\n",
    "        # Input is of form dict[list]\n",
    "        element[\"text\"] = [re.sub(\"\\n\", \" \", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(r\"http\\S+\", \"URL\", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(\"&amp;\", \"&\", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(\"&lt;\", \"<\", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(\"&gt;\", \">\", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(\" +\", \" \", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [\" \".join(txt.split()) for txt in element[\"text\"]]\n",
    "    else:\n",
    "        # Input is of form dict[str]\n",
    "        element[\"text\"] = element[\"text\"]\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a46ae6-46c9-4849-a0e0-92e676321900",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed = dataset_without_rt.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea42dd8-566d-48d9-8fce-825a1548b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed[\"train\"][ids][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab06bb-903e-49db-8d60-63af31caf34d",
   "metadata": {},
   "source": [
    "#### Filter Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffaeaa-f19e-4a63-b712-35ff7929829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_text_length = 50\n",
    "dataset_processed = dataset_processed.filter(\n",
    "    lambda row: len(row[\"text\"]) > min_text_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d47ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.concat(\n",
    "    [\n",
    "        stats,\n",
    "        all_data_pd[all_data_pd[\"text\"].apply(len) <= min_text_length]\n",
    "        .groupby(\"user\")\n",
    "        .size(),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "stats.columns = [\"all_tweets\", \"retweets\", \"short_tweets\"]\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27234746",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79861e-a095-4701-b78b-ea0f7e2179cb",
   "metadata": {},
   "source": [
    "### Create Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682c69b-f383-4ad0-a760-c7059708a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "    element: dict[str] | dict[list], userlist: list[str]\n",
    ") -> dict[str] | dict[list]:\n",
    "    if isinstance(element, datasets.arrow_dataset.Batch):\n",
    "        # Input is of form dict[list]\n",
    "        element[\"text_prompt\"] = [\n",
    "            f\"User: {userlist[label]}\\nTweet: {txt}\"\n",
    "            for txt, label in zip(element[\"text\"], element[\"label\"])\n",
    "        ]\n",
    "    else:\n",
    "        # Input is of form dict[str]\n",
    "        element[\n",
    "            \"text_prompt\"\n",
    "        ] = f\"User: {userlist[element['label']]}\\nTweet: {element['text']}\"\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b849031-d859-4672-ab7b-7069f5a7623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_features = dataset[\"train\"].features[\"label\"].names  # Create List with all users\n",
    "create_prompt_partial = functools.partial(create_prompt, userlist=full_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421a362-c347-442f-8a91-0dcd7300fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc_prompt = dataset_processed.map(create_prompt_partial, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7852b-c291-4022-aeda-2986d7240599",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc_prompt[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e2097-97ec-4296-9815-0ba0aacd878f",
   "metadata": {},
   "source": [
    "### Filter the Users\n",
    "The full dataset contains more users than we want to use for the first trials. Therefore we will reduce the number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473e97c-e30b-464b-b94a-ddcff67e9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_features = [\n",
    "    \"MKBHD\",\n",
    "    \"elonmusk\",\n",
    "    \"katyperry\",\n",
    "    \"neiltyson\",\n",
    "    \"BillGates\",\n",
    "    \"BillNye\",\n",
    "    \"BarackObama\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665fea9-cffa-4fcf-a51f-ca6117d3011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_proc_prompt_filter = dataset_proc_prompt.filter(\n",
    "    lambda row: full_features[row[\"label\"]] in short_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985f535-36d6-4d95-9562-8103d4da313c",
   "metadata": {},
   "source": [
    "### Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf142d-2a4f-48b5-b210-95679fcfcb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_path = os.path.join(\"data\", \"feature\", \"final_dataset\")\n",
    "levels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73a121-a54a-487e-a888-e4f0f97a2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = Path(os.path.abspath(\"\")).parents[levels - 1]\n",
    "feature_dir = os.path.join(parent_path, feature_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458744f8-620f-4d9e-9317-535d6187c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_proc_prompt_filter.save_to_disk(feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3f118-b219-4323-ae0f-f46430eb49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_proc_prompt_filter = load_from_disk(feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d17bb9-a4e0-4a4d-9e9b-35ea5ae9d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datset_proc_prompt_filter[\"train\"]\n",
    "dataset_val = datset_proc_prompt_filter[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead006b3-e785-46a4-aa38-f67d1083f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f2f7e-1f4e-4477-985f-6de337d6c6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1514d5b148675bdeb5bbd073e384df3d5b8241ddd1e2b74f50ac5cc55a81072f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
