{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f735cc35-7762-4152-9ad7-d5d798ff7661",
   "metadata": {},
   "source": [
    "# OpenCampus NLP Project\n",
    "## Tweet Generator for famous Twitter personalities\n",
    "-----------\n",
    "This notebook preprocesses the Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb2299-5456-4411-a087-12a8200eecf7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa6d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update -y\n",
    "!sudo apt-get install python3.10\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33baba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad67299",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf37da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install python3-pip\n",
    "%pip install datasets==2.7.1\n",
    "%pip install transformers==4.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e9a5c-e75b-4dad-99c9-dffab5ccad9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09e556-6f23-4b22-9e17-70074c66079d",
   "metadata": {},
   "source": [
    "## Prepare the dataset for Training\n",
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7132e3-b38f-4ef9-808e-2a02261125bb",
   "metadata": {},
   "source": [
    "First we download our custom HuggingFace (HF) dataset. The dataset can be found on our [HuggingFace site](https://huggingface.co/datasets/ML-Projects-Kiel/tweetyface). It contains Tweets from English and German Twitter users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbf9f8-e3e3-4944-8c68-c579590f0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ML-Projects-Kiel/tweetyface\", \"english\", download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645a965-77e7-4cf0-a411-3594fffc9f14",
   "metadata": {},
   "source": [
    "The dataset already is split into a training and validation subset. It contains no test data, because the text generation task does not require test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190d06a-eb32-41db-85b3-415705ecd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f9a20-271d-4545-b6d9-245cee11cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4760ceb-53f9-47ba-a045-2253e8834e4d",
   "metadata": {},
   "source": [
    "### Preprocess the text\n",
    "\n",
    "#### Remove retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "userlist = dataset[\"train\"].features[\"label\"].names\n",
    "all_data_pd = datasets.concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).to_pandas()\n",
    "all_data_pd[\"user\"] = [userlist[label] for label in all_data_pd[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.concat(\n",
    "    [\n",
    "        all_data_pd.groupby(\"user\").size(),\n",
    "        all_data_pd[all_data_pd[\"text\"].str.contains(\"^RT .*\")].groupby(\"user\").size(),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "stats.columns = [\"all_tweets\", \"retweets\"]\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove retweets\n",
    "dataset_without_rt = dataset.filter(lambda row: not bool(re.search(\"^RT .*\", row[\"text\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d32ba",
   "metadata": {},
   "source": [
    "#### Filter characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708d854-7d7b-49a9-bc84-bfa6e3134338",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = random.sample(range(0, dataset_without_rt[\"train\"].num_rows), 10)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f6e12-9761-403b-82db-d751cb040771",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [12784, 15988, 14475, 10926, 23442, 6853, 22511, 18022, 13039, 22725]\n",
    "dataset_without_rt[\"train\"][ids][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb19156-1c2d-4625-a208-a1faed054c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(element):\n",
    "    if isinstance(element, datasets.arrow_dataset.Batch):\n",
    "        # Input is of form dict[list]\n",
    "        element[\"text\"] = [re.sub(\"\\n\", \" \", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(r\"http\\S+\", \"URL\", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(\"&amp;\", \"&\", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(\"&lt;\", \"<\", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(\"&gt;\", \">\", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [re.sub(\" +\", \" \", txt) for txt in element[\"text\"]]\n",
    "        element[\"text\"] = [\" \".join(txt.split()) for txt in element[\"text\"]]\n",
    "    else:\n",
    "        # Input is of form dict[str]\n",
    "        element[\"text\"] = element[\"text\"]\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a46ae6-46c9-4849-a0e0-92e676321900",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed = dataset_without_rt.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea42dd8-566d-48d9-8fce-825a1548b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed[\"train\"][ids][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab06bb-903e-49db-8d60-63af31caf34d",
   "metadata": {},
   "source": [
    "#### Filter Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffaeaa-f19e-4a63-b712-35ff7929829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_text_length = 50\n",
    "dataset_processed = dataset_processed.filter(lambda row: len(row[\"text\"]) > min_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d47ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.concat(\n",
    "    [\n",
    "        stats,\n",
    "        all_data_pd[all_data_pd[\"text\"].apply(len) <= min_text_length].groupby(\"user\").size(),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "stats.columns = [\"all_tweets\", \"retweets\", \"short_tweets\"]\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27234746",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79861e-a095-4701-b78b-ea0f7e2179cb",
   "metadata": {},
   "source": [
    "### Create Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682c69b-f383-4ad0-a760-c7059708a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(element):\n",
    "    if isinstance(element, datasets.arrow_dataset.Batch):\n",
    "        # Input is of form dict[list]\n",
    "        element[\"text_prompt\"] = [\n",
    "            f\"User: {userlist[label]}\\nTweet: {txt}\"\n",
    "            for txt, label in zip(element[\"text\"], element[\"label\"])\n",
    "        ]\n",
    "    else:\n",
    "        # Input is of form dict[str]\n",
    "        element[\"text_prompt\"] = f\"User: {userlist[element['label']]}\\nTweet: {element['text']}\"\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b849031-d859-4672-ab7b-7069f5a7623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_features = dataset[\"train\"].features[\"label\"].names  # Create List with all users\n",
    "create_prompt_partial = functools.partial(create_prompt, userlist=full_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421a362-c347-442f-8a91-0dcd7300fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc_prompt = dataset_processed.map(create_prompt_partial, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7852b-c291-4022-aeda-2986d7240599",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc_prompt[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e2097-97ec-4296-9815-0ba0aacd878f",
   "metadata": {},
   "source": [
    "### Filter the Users\n",
    "The full dataset contains more users than we want to use for the first trials. Therefore we will reduce the number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473e97c-e30b-464b-b94a-ddcff67e9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_features = [\n",
    "    \"elonmusk\",\n",
    "    \"neiltyson\",\n",
    "    \"BillGates\",\n",
    "    \"BillNye\",\n",
    "    \"BarackObama\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665fea9-cffa-4fcf-a51f-ca6117d3011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc_prompt_filter = dataset_proc_prompt.filter(\n",
    "    lambda row: full_features[row[\"label\"]] in short_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985f535-36d6-4d95-9562-8103d4da313c",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f2f7e-1f4e-4477-985f-6de337d6c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c5b9d-8e56-4ee4-8593-3434c63f5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # , return_special_tokens_mask=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d177d4c-27c3-4cac-a9c4-2afee74abe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36335505-eea6-4fad-91d1-6618b0f193bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9499868-b3aa-4689-871a-7b0af4c7879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc_prompt_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c622436-51e4-4058-b505-5de1798437a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max([len(txt) for txt in dataset_proc_prompt_filter[\"train\"][\"text_prompt\"]]))\n",
    "print(max([len(txt) for txt in dataset_proc_prompt_filter[\"validation\"][\"text_prompt\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45384c-f5d1-4ffc-a484-79c6fa5f926f",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb80bf4-e907-469a-94b5-2862f93c4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eac646-ce8a-430a-a7b6-c28163dd3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset_proc_prompt_filter.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"text\", \"label\", \"idx\", \"ref_tweet\", \"reply_tweet\", \"text_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29e928-29b3-4b8c-9fc7-dbba97a06f3f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d13655-f2a0-42e2-a5da-3920460ad56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1.372e-4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../../model/\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=0,\n",
    "    seed=20,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0815034-e22b-429b-9561-44b03cf45b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f41e8c-7a9a-4b18-ac99-e10c0d4f7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCampusNLP",
   "language": "python",
   "name": "opencampusnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1514d5b148675bdeb5bbd073e384df3d5b8241ddd1e2b74f50ac5cc55a81072f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
