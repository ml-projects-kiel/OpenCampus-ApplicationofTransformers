{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Transformer architecture was introduced in 2017. The most influential models were:\n",
    "- GPT, BERT (2018)\n",
    "- GPT-2, DistilBERT, BART, T5 (2019)\n",
    "- GPT-3 (2020) -> allows zero-shot learning\n",
    "\n",
    "Different kinds of Transformer models:\n",
    "- GPT-like (also called auto-regressive Transformer models)\n",
    "- BERT-like (also called auto-encoding Transformer models)\n",
    "- BART/T5-like (also called sequence-to-sequence Transformer models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "<u>Self-supervised</u><br>\n",
    "Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data! This type of model develops a statistical understanding of the language it has been trained on, but itâ€™s not very useful for specific practical tasks.\n",
    "Requires <u>transfer learning</u> on labelled data.\n",
    "\n",
    "<u>Pretraining</u><br>\n",
    "Training a model from scratch. Weights are randomly initialized.\n",
    "\n",
    "<u>Transfer learning</u><br>\n",
    "Fine-tune a pretrained model. Requires less data than training from scratch. Often resulting in better results. Replace the Head of the pretrained model (e.g. change number of output labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "### Encoder-only\n",
    "The encoder receives an input and creates a feature vector/tensor.\n",
    "\n",
    "Good for natural language understanding (NLU).\n",
    "Used for tasks:\n",
    "- Sentence classification\n",
    "- Named entity recognition\n",
    "- Question answering\n",
    "- Fill mask\n",
    "- Sentiment analysis\n",
    "\n",
    "Attention layers can access all the words in the initial sentence (\"bi-directional\" attention). Are often called auto-encoding models. Good at extracting meaningful information. Good at obtaining an understanding of sequences; and the relationship/interdependence between words.\n",
    "\n",
    "Models include:\n",
    "- ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder-only\n",
    "Receives an input and generates a feature vector/tensor (sequence). Can perform most of the same tasks as an Encoder, but with worse performance.\n",
    "\n",
    "Good for causal language modeling or natural language generation (NLG).\n",
    "Used for tasks:\n",
    "- Text generation\n",
    "\n",
    "Right context of a word is \"masked\". Attention layers can only access the words positioned before in the sentence (\"uni-directional\" attention). Are often called auto-regressive models. Use their past outputs as new inputs for the next output.\n",
    "\n",
    "Models include:\n",
    "- CTRL, GPT, GPT-2, Transformer XL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder (Transformer)\n",
    "\n",
    "Hyperparameters:\n",
    "- Nx: Number of Encoder blocks\n",
    "- Positional Encoding: How the positional encodings are created.\n",
    "\n",
    "Used for tasks:\n",
    "- Translation\n",
    "- Summarization\n",
    "\n",
    "Also called sequence-to-sequence models. Can perform the tasks of encoder and decoder models, but usually involves more complex tasks. Best for generating new sentences depending on a given input.\n",
    "\n",
    "Models include:\n",
    "- BART, mBART, Marian, T5\n",
    "\n",
    "<img src=\"images/transformer_architecture.png\" style=\"width:400px\">\n",
    "\n",
    "Junctions for the input prevent vanishing gradients through multiple layers.\n",
    "\n",
    "<u>Positional Encoding</u>:<br>\n",
    "Attention does not care about the order of the input. Positional encoding tries to fix that problem. Input-Encodings and Positional Encodings are added together. \n",
    "- Can be learned.\n",
    "- Can use a function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Scaled Dot-Product Attention</u>:<br>\n",
    "Similar to attention mechanism. But uses an additional scaling step for numeric stability during gradient descent.\n",
    "\n",
    "<img src=\"images/scaled_dot_product_attention.png\" style=\"width:200px\">\n",
    "\n",
    "<u>Multi-Head Attention</u>:<br>\n",
    "Also uses Values, Keys and Queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "Consists of three different stages:<br>\n",
    "Tokenizer -> Model -> Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot-classification\n",
    "\n",
    "Any amount of new labels can be provided. The model returns the probabilities for each label. No fine-tune needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\", \"data science\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generator(\"In this course, we will teach you how to\", num_return_sequences=3, max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A specific model can be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"sberbank-ai/mGPT\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask-filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\")\n",
    "unmasker(\"This course will teach you all about [MASK] models.\", top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named-Entity-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\", min_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer & Model & Postprocessing\n",
    "### Tokenizer\n",
    "\n",
    "- Split text into tokens (words, part of words, punctuation symbols)\n",
    "- Add special tokens\n",
    "- Match each token with unique ID in vocabulary in pretrained model\n",
    "\n",
    "Different forms of tokenizers:\n",
    "- Word-based have ids for words. Can grow very large or have many unknown words.\n",
    "- Character-based have ids for characters. Smaller than word-based, but characters do not hold as much information as words. More tokens are created and limit the input length.\n",
    "- Subword-based mixture of word- and character-based. Split words into meaningful subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"Why are you so [MASK]?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(raw_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(raw_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(raw_inputs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, max_length=8, return_tensors=\"pt\")\n",
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs[\"input_ids\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "- Convert Token IDs into embeddings\n",
    "- Attention mechanism layers produce hidden state (high dimensionality)\n",
    "- Head projects the hidden state into a different dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration\n",
    "- Load Model architecture from pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, DistilBertModel, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_bert_config = AutoConfig.from_pretrained(checkpoint, n_layers=2)\n",
    "distil_bert_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize model with config architecture, but random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_bert_model = DistilBertModel(distil_bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_bert_model.save_pretrained(\"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DistilBertModel.from_pretrained(\"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DistilBertForSequenceClassification.from_pretrained(\"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCampusNLP",
   "language": "python",
   "name": "opencampusnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "263a728dd34e9ebcb54f85026436edea16a1f8f3ee11b370480b198503dd32a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
