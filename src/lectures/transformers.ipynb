{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Transformer architecture was introduced in 2017. The most influential models were:\n",
    "- GPT, BERT (2018)\n",
    "- GPT-2, DistilBERT, BART, T5 (2019)\n",
    "- GPT-3 (2020) -> allows zero-shot learning\n",
    "\n",
    "Different kinds of Transformer models:\n",
    "- GPT-like (also called auto-regressive Transformer models)\n",
    "- BERT-like (also called auto-encoding Transformer models)\n",
    "- BART/T5-like (also called sequence-to-sequence Transformer models)\n",
    "\n",
    "## Training\n",
    "\n",
    "<u>Self-supervised</u><br>\n",
    "Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data! This type of model develops a statistical understanding of the language it has been trained on, but itâ€™s not very useful for specific practical tasks.\n",
    "Requires <u>transfer learning</u> on labelled data.\n",
    "\n",
    "<u>Pretraining</u><br>\n",
    "Training a model from scratch. Weights are randomly initialized.\n",
    "\n",
    "<u>Transfer learning</u><br>\n",
    "Fine-tune a pretrained model. Requires less data than training from scratch. Often resulting in better results. Replace the Head of the pretrained model (e.g. change number of output labels).\n",
    "\n",
    "## Architecture\n",
    "### Encoder-only\n",
    "The encoder receives an input and creates a feature vector/tensor.\n",
    "\n",
    "Good for natural language understanding (NLU).\n",
    "Used for tasks:\n",
    "- Sentence classification\n",
    "- Named entity recognition\n",
    "- Question answering\n",
    "- Fill mask\n",
    "- Sentiment analysis\n",
    "\n",
    "Attention layers can access all the words in the initial sentence (\"bi-directional\" attention). Are often called auto-encoding models. Good at extracting meaningful information. Good at obtaining an understanding of sequences; and the relationship/interdependence between words.\n",
    "\n",
    "Models include:\n",
    "- ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa\n",
    "\n",
    "### Decoder-only\n",
    "Receives an input and generates a feature vector/tensor (sequence). Can perform most of the same tasks as an Encoder, but with worse performance.\n",
    "\n",
    "Good for causal language modeling or natural language generation (NLG).\n",
    "Used for tasks:\n",
    "- Text generation\n",
    "\n",
    "Right context of a word is \"masked\". Attention layers can only access the words positioned before in the sentence (\"uni-directional\" attention). Are often called auto-regressive models. Use their past outputs as new inputs for the next output.\n",
    "\n",
    "Models include:\n",
    "- CTRL, GPT, GPT-2, Transformer XL\n",
    "\n",
    "### Encoder-Decoder\n",
    "\n",
    "Used for tasks:\n",
    "- Translation\n",
    "- Summarization\n",
    "\n",
    "Also called sequence-to-sequence models. Can perform the tasks of encoder and decoder models, but usually involves more complex tasks. Best for generating new sentences depending on a given input.\n",
    "\n",
    "Models include:\n",
    "- BART, mBART, Marian, T5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot-classification\n",
    "\n",
    "Any amount of new labels can be provided. The model returns the probabilities for each label. No fine-tune needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\", \"data science\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generator(\"In this course, we will teach you how to\", num_return_sequences=3, max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A specific model can be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"sberbank-ai/mGPT\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask-filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\")\n",
    "unmasker(\"This course will teach you all about [MASK] models.\", top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named-Entity-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\", min_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "263a728dd34e9ebcb54f85026436edea16a1f8f3ee11b370480b198503dd32a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
